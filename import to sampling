import pandas as pd
import numpy as np #for random sampling


#file import / export
data = pd.read_csv('results.tsv', sep='\t', names=('id','unixtime','url')) #import
data.to_csv("path_of_exported_file") #export

#show limited rows
pd.options.display.max_rows=10


data.columns   #列名を取得
data.index     #インデックス名を取得

data.columns = ["day_no","class","point1","point2"]   # カラム名を上書き
data.index   = [11,12,13,14,15,16]   # インデックス名を上書きする

len(df_sample) # 行数の確認
df_sample.shape #次元数の確認:（行数、列数）の形で返す
df_sample.info() #カラム名とその型の一覧

# 各列の基礎統計量の確認
df_sample.describe() # 平均、分散、4分位など. Rでいうところのsummary()

# head / tail
df_sample.head(10) #先頭10行を確認
df_sample.tail(10) #先頭10行を確認

print(data['url'])
data["url"]
data.url
data[['id','url']]

data[1:5]
data[-2:]
data[:3]

data.ix[1:5,'url'] # .ixは.loc/.ilocと互換性あり

data.loc[:,['id','url']]
data.loc[13:16,['id','url']]
data.loc[13:16,:]

data.iloc[[1,2,4],[0,2]]
data.iloc[1:3,:]

data[data.url=='https://hub.docker.com/']
data[data.id==13]

data[data.id.isin([1,2,3,4])]
data[data.url.isin(['https://www.google.co.jp/','https://hub.docker.com/'])]
#data.query("url=='https://www.google.co.jp/'|url=='https://hub.docker.com/'")

data.sort('url')  #SORT

data.xxxx.value_counts().sort_index()  #frequency count for histgram
(xxxx : column name)

pd.concat([data,data_to_add],axis=0) #CONCATENATE =rbind. Axis=0で縦方向,axis=1は横方向の結合を指定

#SAMPLING
sampler = np.random.permutation(len(data))
data.take(sampler[:10])
